# -*- coding=utf-8 -*-
# @Time: 2025/10/21 14:10
# @Author: é‚±æ¥ 
# @File: excel_to_vector_db.py.py
# @Software: PyCharm


import os
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import chromadb
from chromadb.config import Settings
import glob
import json
from tqdm import tqdm
import re

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'


class ExcelToVectorDB:
    def __init__(self, model_name='BAAI/bge-small-zh', db_path="./vector_db", collection_name="qa_embeddings_bge"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç³»ç»Ÿ

        Args:
            model_name: æ¨¡å‹åç§°
            db_path: æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.db_path = db_path
        self.collection_name = collection_name

        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=db_path)

        # åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "é—®ç­”å¯¹å‘é‡æ•°æ®åº“_bge"}
        )
        print(f"âœ“ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ! ä½¿ç”¨é›†åˆ: {collection_name}")

    def list_collections(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é›†åˆ"""
        collections = self.client.list_collections()
        collection_names = [col.name for col in collections]
        print(f"ğŸ“ æ•°æ®åº“ä¸­çš„é›†åˆ: {collection_names}")
        return collection_names

    def read_excel_files(self, excel_folder, file_pattern="*.xlsx"):
        """
        è¯»å–å¤šä¸ªExcelæ–‡ä»¶ä¸­çš„é—®ç­”å¯¹
        """
        print(f"æ­£åœ¨è¯»å–Excelæ–‡ä»¶ä»: {excel_folder}")

        # æŸ¥æ‰¾æ‰€æœ‰Excelæ–‡ä»¶
        excel_files = glob.glob(os.path.join(excel_folder, file_pattern))
        excel_files.extend(glob.glob(os.path.join(excel_folder, "*.xls")))

        if not excel_files:
            raise ValueError(f"åœ¨ {excel_folder} ä¸­æœªæ‰¾åˆ°Excelæ–‡ä»¶")

        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")

        all_qa_pairs = []

        for file_path in excel_files:
            print(f"æ­£åœ¨å¤„ç†: {os.path.basename(file_path)}")

            try:
                # è¯»å–Excelæ–‡ä»¶
                excel_data = pd.read_excel(file_path)

                # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
                for idx, row in excel_data.iterrows():
                    chunk_id = str(row["chunk_id"]).strip()
                    header = str(row["header"]).strip()
                    raw_content = str(row["content"]).strip()
                    standard_question = str(row["æ ‡å‡†é—®é¢˜"]).strip()
                    extended_question = str(row["å‘æ•£é—®é¢˜"]).strip()
                    standard_answer = str(row["å®¢æœç­”æ¡ˆ"]).strip()
                    image_url = str(row["å›¾ç‰‡"]).strip()

                    qa_pair = {
                        "chunk_id": chunk_id,
                        "header": header,
                        "raw_content": raw_content,
                        "standard_question": standard_question,
                        "extended_question": extended_question,
                        "standard_answer": standard_answer,
                        "image_url": image_url,
                        "file_source": os.path.basename(file_path),
                        "row_index": idx
                    }
                    all_qa_pairs.append(qa_pair)

                print(
                    f"  âœ“ ä» {os.path.basename(file_path)} æå–äº† {len([p for p in all_qa_pairs if p['file_source'] == os.path.basename(file_path)])} ä¸ªé—®ç­”å¯¹")

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                continue

        print(f"âœ“ æ€»å…±æå–äº† {len(all_qa_pairs)} ä¸ªé—®ç­”å¯¹")
        return all_qa_pairs

    def process_and_store_embeddings(self, excel_folder, batch_size=100):
        """
        å¤„ç†é—®ç­”å¯¹å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        """
        # è¯»å–é—®ç­”å¯¹
        qa_pairs = self.read_excel_files(excel_folder)

        if not qa_pairs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„é—®ç­”å¯¹")
            return

        print("æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“...")
        seen_questions = set()

        for i in tqdm(range(0, len(qa_pairs), batch_size)):
            batch = qa_pairs[i:i + batch_size]

            # å‡†å¤‡é—®é¢˜å’Œå¯¹åº”çš„å…ƒæ•°æ®
            questions = []
            metadatas = []

            for item in batch:
                # æ·»åŠ æ ‡å‡†é—®é¢˜
                standard_question = item["standard_question"]
                if item["standard_question"] not in seen_questions:
                    seen_questions.add(standard_question)
                    questions.append(item["standard_question"])
                    metadatas.append({
                        "chunk_id": item["chunk_id"],
                        "header": item["header"],
                        "raw_content": item["raw_content"],
                        "standard_answer": item["standard_answer"],
                        "image_url": item["image_url"],
                        "file_source": item["file_source"],
                        "row_index": item["row_index"],
                        "question_type": "standard"  # æ ‡è®°ä¸ºæ ‡å‡†é—®é¢˜
                    })

                # æ·»åŠ å‘æ•£é—®é¢˜ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼‰
                if item["extended_question"] and item["extended_question"] != 'nan':
                    questions.append(item["extended_question"])
                    metadatas.append({
                        "chunk_id": item["chunk_id"],
                        "header": item["header"],
                        "raw_content": item["raw_content"],
                        "standard_answer": item["standard_answer"],
                        "image_url": item["image_url"],
                        "file_source": item["file_source"],
                        "row_index": item["row_index"],
                        "question_type": "extended"  # æ ‡è®°ä¸ºå‘æ•£é—®é¢˜
                    })

            if not questions:
                continue

            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = self.model.encode(questions)
            embeddings = normalize(embeddings)

            # å‡†å¤‡æ•°æ® - ç¡®ä¿æ‰€æœ‰å­—æ®µé•¿åº¦ä¸€è‡´
            ids = [f"qa_{i + j}" for j in range(len(questions))]
            documents = questions  # æ–‡æ¡£å°±æ˜¯é—®é¢˜æœ¬èº«
            # print(documents)

            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )

        print(f"âœ“ æˆåŠŸå­˜å‚¨ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹åˆ°å‘é‡æ•°æ®åº“")

    def search_similar_questions(self, query, top_k=5, min_score=0.5):
        """
        æœç´¢ç›¸ä¼¼é—®é¢˜
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.model.encode([query])[0]
        query_embedding = normalize([query_embedding])[0]

        # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )

        formatted_results = []
        if results['documents']:
            for i in range(len(results['documents'][0])):
                distance = results['distances'][0][i]
                score = 1 - distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°

                if score >= min_score:
                    formatted_results.append({
                        'question': results['documents'][0][i],
                        'chunk_id': results['metadatas'][0][i]['chunk_id'],
                        'header': results['metadatas'][0][i]['header'],
                        'raw_content': results['metadatas'][0][i]['raw_content'],
                        'standard_answer': results['metadatas'][0][i]['standard_answer'],
                        'image_url': results['metadatas'][0][i]['image_url'],
                        'file_source': results['metadatas'][0][i]['file_source'],
                        'score': float(score)
                    })

        return formatted_results

    def get_collection_stats(self):
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        return self.collection.count()

    def export_qa_data(self, output_file="qa_data_export.json"):
        """å¯¼å‡ºé—®ç­”æ•°æ®"""
        try:
            all_data = self.collection.get()

            if not all_data['ids']:
                print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®")
                return

            qa_list = []
            for i in range(len(all_data['ids'])):
                # å®‰å…¨åœ°è®¿é—®å…ƒæ•°æ®
                metadata = all_data['metadatas'][i] if i < len(all_data['metadatas']) else {}

                qa_list.append({
                    "id": all_data['ids'][i],
                    "chunk_id": metadata.get('chunk_id', ''),
                    "header": metadata.get('header', ''),
                    "raw_content": metadata.get('raw_content', ''),
                    "question": all_data['documents'][i] if i < len(all_data['documents']) else '',
                    "standard_answer": metadata.get('standard_answer', ''),
                    "file_source": metadata.get('file_source', ''),
                    "question_type": metadata.get('question_type', 'unknown'),
                    "image_url": metadata.get('image_url', '')
                })

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(qa_list, f, ensure_ascii=False, indent=2)

            print(f"âœ“ é—®ç­”æ•°æ®å·²å¯¼å‡ºåˆ° {output_file}ï¼Œå…± {len(qa_list)} æ¡è®°å½•")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºæ•°æ®æ—¶å‡ºé”™: {e}")


def main():
    # åˆå§‹åŒ–ç³»ç»Ÿ
    vector_db = ExcelToVectorDB()

    # æŒ‡å®šExcelæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
    excel_folder = "./excel_files"  # ä¿®æ”¹ä¸ºä½ çš„Excelæ–‡ä»¶å¤¹è·¯å¾„

    # å¤„ç†Excelæ–‡ä»¶å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    vector_db.process_and_store_embeddings(excel_folder)

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    count = vector_db.get_collection_stats()
    print(f"\nğŸ“Š å‘é‡æ•°æ®åº“ä¸­çš„é—®ç­”å¯¹æ•°é‡: {count}")

    # æµ‹è¯•æœç´¢åŠŸèƒ½
    test_queries = [
        "æ™ºå°Šç‰ˆLæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "æ˜¯å¦æœ‰ç‰¹å®šçš„æ¸©åº¦æˆ–æ¹¿åº¦è¦æ±‚ï¼Ÿ",
        "ä½æ²¹è€—æŠ€æœ¯ä¼šæœ‰ä»€ä¹ˆå½±å“å—å¯¹ç¯å¢ƒï¼Ÿ"
    ]

    print("\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½:")
    print("=" * 60)

    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = vector_db.search_similar_questions(query, top_k=3)

        if results:
            for i, result in enumerate(results, 1):
                print(f"  {i}. ç›¸ä¼¼åº¦: {result['score']:.4f}")
                print(f"     é—®é¢˜: {result['question']}")
                print(f"     æ¥æº: {result['file_source'], result['chunk_id'],result['header']}")
                print(f"     åŸå§‹æ•°æ®: {result['raw_content']}")
                print(f"     ç­”æ¡ˆ: {result['standard_answer']}")
        else:
            print("  âŒ æœªæ‰¾åˆ°ç›¸å…³ç­”æ¡ˆ")

        print("-" * 60)

    # å¯¼å‡ºæ•°æ®ï¼ˆå¯é€‰ï¼‰
    vector_db.export_qa_data()


if __name__ == "__main__":
    main()